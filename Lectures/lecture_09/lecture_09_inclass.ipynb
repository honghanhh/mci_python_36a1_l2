{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5869783-46cd-4ef5-8d80-b1c6eaf559bd",
   "metadata": {},
   "source": [
    "# Lecture 09 - Nature Language Processing (NLP) & Text Mining\n",
    "\n",
    "![](https://www.thuatngumarketing.com/wp-content/uploads/2017/12/NLP.png.pagespeed.ce_.1YNuw_5dJH.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372869b4-6785-4aaa-8abc-3e29d875fb0c",
   "metadata": {},
   "source": [
    "## Definition - Định nghĩa\n",
    "\n",
    "Sử dụng các kỹ thuật phân tích và làm sạch dữ liệu kết hợp với mô hình machine learning để khai thác thông tin trong dữ liệu ngôn ngữ.\n",
    "\n",
    "Dữ liệu ngôn ngữ?\n",
    "- Nói cho nhau nghe (dữ liệu ngôn ngữ âm thanh)\n",
    "- Viết cho nhau đọc (dữ liệu ngôn ngữ văn bản)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa5f21f-79fa-47a0-bc91-f1828d658b21",
   "metadata": {},
   "source": [
    "Kỹ thuật xử lý dữ liệu dạng văn bản và khai thác thông tin từ dữ liệu văn bản: Text Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1ff90f-7126-4e50-8272-e7bb5ebc5092",
   "metadata": {},
   "source": [
    "## Flow - Quy trình\n",
    "\n",
    "1. Preprocessing (Tiền xử lý) (Clean & Normalize)\n",
    "2. Tokenizing (tách từ)\n",
    "3. Vectorizing (Vectơ hóa)\n",
    "4. Modeling (Xây dựng mô hình)\n",
    "5. Intepreting result & Application (Ứng dụng)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36aab243-6557-4d52-8327-c3b96dbcda2e",
   "metadata": {},
   "source": [
    "## Application - Ứng dụng\n",
    "\n",
    "1. Sentimental Analysis (Phân tích cảm tính): phân tích reviews, phân tích chứng khoán (sử dụng nền tảng Twitter)\n",
    "2. Search suggestion\n",
    "3. Speech recognition \n",
    "4. Etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de5b914-9acd-4e9f-9072-a6663fea3313",
   "metadata": {},
   "source": [
    "## Tools & Library\n",
    "\n",
    "1. Python Nature Language Toolkit (Python NLTK) # Xử lý dữ liệu dạng text cho mô hình text mining\n",
    "2. Gensim/Tensorflow/Scikit-learn etc.  # Xây dựng mô hình text mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6809b94d-884f-4854-8be2-ffbe60b749c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd1341a-bd09-4e48-ae1e-ede6ff8565df",
   "metadata": {},
   "outputs": [],
   "source": [
    "string.split()\n",
    "string.trim()\n",
    "string.lower()\n",
    "string.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "290c12bf-f701-4a6f-ada0-78c0a0be0acf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'revolation'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"cách mạng\" # 1 từ\n",
    "\"revolation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85fc669d-4cbc-4939-a73e-e5057de2bb8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the, a, an, in, out'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"the, a, an, in, out\" # từ đệm # bản thân ko mang nhiều ý nghĩa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0437f3-5113-472a-ab06-01f3999c4002",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fe2646-68db-485a-a3d7-9c5c3e3c2bb5",
   "metadata": {},
   "source": [
    "### Token hóa dữ liệu text (Text tokenization)\n",
    "\n",
    "Là quá trình làm sạch và biến đổi dữ liệu text thành dạng sẵn sàng đưa vào mô hình"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da8ab72-48f2-4e8d-b889-1f8637c81797",
   "metadata": {},
   "source": [
    "#### Tiền xử lý dữ liệu text (Text preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80020240-0711-4916-b200-3b76dc1e096f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>snippet</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thủ tướng: Khác với đa số các nước, dư địa chí...</td>\n",
       "      <td>(Tổ Quốc) - Với bối cảnh hiện nay, Hội đồng Tư...</td>\n",
       "      <td>Sáng ngày 9/7, Hội đồng Tư vấn chính sách tài ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thống nhất kịch bản tăng trưởng 3-4%, lạm phát...</td>\n",
       "      <td>Tại cuộc họp Hội đồng Tư vấn chính sách tài ch...</td>\n",
       "      <td>Tham dự cuộc họp có Thống đốc NHNN Lê Minh Hưn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Đề xuất tăng liều lượng gói kích thích kinh tế...</td>\n",
       "      <td>Chuyên gia đề xuất do dịch bệnh Covid-19 còn k...</td>\n",
       "      <td>Sáng 9/7, Thủ tướng Nguyễn Xuân Phúc chủ trì h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Thủ tướng: Khác với đa số các nước, dư địa chí...</td>\n",
       "      <td>Với bối cảnh hiện nay, Hội đồng Tư vấn chính s...</td>\n",
       "      <td>Với bối cảnh hiện nay, Hội đồng Tư vấn chính s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tác động từ chính sách tín dụng: BĐS có thể đó...</td>\n",
       "      <td>Động thái thắt chặt hay nới lỏng của chính sác...</td>\n",
       "      <td>Lời tòa soạn Thị trường bất động sản ngày càng...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871</th>\n",
       "      <td>Việt Nam thành lập Viện nghiên cứu phát triển ...</td>\n",
       "      <td>Sáng 2/7, Đại học Quốc gia Thành phố Hồ Chí Mi...</td>\n",
       "      <td>Phát biểu tại buổi lễ, PGS.TS Huỳnh Thanh Đạt,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>872</th>\n",
       "      <td>Bất động sản chờ lực bật mới</td>\n",
       "      <td>Chịu nhiều khó khăn do tác động từ đại dịch CO...</td>\n",
       "      <td>Nhu cầu thị trường nhà ở của người dân thuộc p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>873</th>\n",
       "      <td>Diễn đàn Bất động sản 2020: Cơ hội mới từ chín...</td>\n",
       "      <td>Diễn đàn Bất động sản 2020: Cơ hội mới từ chín...</td>\n",
       "      <td>BVR&amp;MT – Năm 2020, Thị trường bất động sản có ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>874</th>\n",
       "      <td>Thống đốc Lê Minh Hưng: Tín dụng tăng trở lại,...</td>\n",
       "      <td>Thống đốc Lê Minh Hưng: Tín dụng tăng trở lại,...</td>\n",
       "      <td>Đến ngày 29/6 tín dụng tăng 3,26%, là mức tăng...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>Sẵn sàng đón nhận dòng vốn đầu tư dịch chuyển ...</td>\n",
       "      <td>Trong 6 tháng, Việt Nam cơ bản thực hiện thành...</td>\n",
       "      <td>Bộ trưởng Bộ Kế hoạch và Đầu tư Nguyễn Chí Dũn...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>876 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title  \\\n",
       "0    Thủ tướng: Khác với đa số các nước, dư địa chí...   \n",
       "1    Thống nhất kịch bản tăng trưởng 3-4%, lạm phát...   \n",
       "2    Đề xuất tăng liều lượng gói kích thích kinh tế...   \n",
       "3    Thủ tướng: Khác với đa số các nước, dư địa chí...   \n",
       "4    Tác động từ chính sách tín dụng: BĐS có thể đó...   \n",
       "..                                                 ...   \n",
       "871  Việt Nam thành lập Viện nghiên cứu phát triển ...   \n",
       "872                       Bất động sản chờ lực bật mới   \n",
       "873  Diễn đàn Bất động sản 2020: Cơ hội mới từ chín...   \n",
       "874  Thống đốc Lê Minh Hưng: Tín dụng tăng trở lại,...   \n",
       "875  Sẵn sàng đón nhận dòng vốn đầu tư dịch chuyển ...   \n",
       "\n",
       "                                               snippet  \\\n",
       "0    (Tổ Quốc) - Với bối cảnh hiện nay, Hội đồng Tư...   \n",
       "1    Tại cuộc họp Hội đồng Tư vấn chính sách tài ch...   \n",
       "2    Chuyên gia đề xuất do dịch bệnh Covid-19 còn k...   \n",
       "3    Với bối cảnh hiện nay, Hội đồng Tư vấn chính s...   \n",
       "4    Động thái thắt chặt hay nới lỏng của chính sác...   \n",
       "..                                                 ...   \n",
       "871  Sáng 2/7, Đại học Quốc gia Thành phố Hồ Chí Mi...   \n",
       "872  Chịu nhiều khó khăn do tác động từ đại dịch CO...   \n",
       "873  Diễn đàn Bất động sản 2020: Cơ hội mới từ chín...   \n",
       "874  Thống đốc Lê Minh Hưng: Tín dụng tăng trở lại,...   \n",
       "875  Trong 6 tháng, Việt Nam cơ bản thực hiện thành...   \n",
       "\n",
       "                                               content  \n",
       "0    Sáng ngày 9/7, Hội đồng Tư vấn chính sách tài ...  \n",
       "1    Tham dự cuộc họp có Thống đốc NHNN Lê Minh Hưn...  \n",
       "2    Sáng 9/7, Thủ tướng Nguyễn Xuân Phúc chủ trì h...  \n",
       "3    Với bối cảnh hiện nay, Hội đồng Tư vấn chính s...  \n",
       "4    Lời tòa soạn Thị trường bất động sản ngày càng...  \n",
       "..                                                 ...  \n",
       "871  Phát biểu tại buổi lễ, PGS.TS Huỳnh Thanh Đạt,...  \n",
       "872  Nhu cầu thị trường nhà ở của người dân thuộc p...  \n",
       "873  BVR&MT – Năm 2020, Thị trường bất động sản có ...  \n",
       "874  Đến ngày 29/6 tín dụng tăng 3,26%, là mức tăng...  \n",
       "875  Bộ trưởng Bộ Kế hoạch và Đầu tư Nguyễn Chí Dũn...  \n",
       "\n",
       "[876 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_json('data.json')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e22f057-8b1b-4898-a768-6085564bf755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Thủ tướng: Khác với đa số các nước, dư địa chính sách tài khóa, tiền tệ của Việt Nam còn khá lớn'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['title'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "651749cb-7a5f-4843-b816-67b5a34effde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Làm sạch dữ liệu chỉ giữ lại dữ liệu bằng chữ\n",
    "# 1. Nếu là dữ liệu scrapped từ web thì chỉ trích lọc dữ liệu chứ (text only)\n",
    "# 2. Loại bỏ hyperlink (nếu có) # https://baophapluat.com # lẫn thành từ word \n",
    "# 3. Nếu là dữ liệu web thì cần loại bỏ emoji # :smile: :angry:\n",
    "# 4. Loại bỏ tất cả các dấu (.,\\/!@#$%^&*()+_ etc.)\n",
    "# 5. Loại bỏ tất cả các số\n",
    "# 6. Loại bỏ các khoảng trắng và đổi text thành lowercase # normalize # T == t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "703cf364-898e-4494-8640-a16d3cea1643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in c:\\users\\tdoan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\tdoan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from bs4) (4.9.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\tdoan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from beautifulsoup4->bs4) (2.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4 # BeautifulSoup => đọc document: Đọc dữ liệu trang web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5959931b-a401-4a4b-9cc9-fd43e996ae8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.\n",
    "import bs4 \n",
    "\n",
    "def del_html(text):\n",
    "    soup = bs4.BeautifulSoup(text)\n",
    "    return soup.get_text(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62c1e331-1dcf-402b-8af8-b36629a639c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2.\n",
    "import re # regular expression\n",
    "\n",
    "def del_link(text):\n",
    "    link = r'http[\\S]*' # pattern (quy tắc) # đấu sao (*) thay cho ký tự bất kỳ\n",
    "    text = re.sub(link, ' ', str(text))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4dee2e0-604d-4c64-ae3e-84cfb8eb648c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: demoji in c:\\users\\tdoan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (1.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install demoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04be7513-0bb3-4e8f-b7e6-3f517d5a8536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.\n",
    "import demoji\n",
    "\n",
    "def del_emoji(text):\n",
    "    return demoji.replace(text, '') # thây thế tất cả emoji (:smile:) bằng ký tự trống "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b3915e9-e245-453d-9f89-ba460d5dbc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.\n",
    "def del_punctuation(doc):\n",
    "    pattern = r'[\\,\\.\\/\\\\\\!\\@\\#\\+\\\"\\'\\;\\)\\(\\“\\”\\\\\\-\\:…&><=\\-\\%\\|\\^\\$\\&\\)\\(\\[\\]\\{\\}\\?\\*\\•]'\n",
    "    record = re.sub(pattern, ' ', doc)\n",
    "    return re.sub(r'\\n', ' ', record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa243430-1dfd-4556-b5a8-5d762c3e4fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.\n",
    "def del_numbers(text):\n",
    "    return re.sub(r'\\d+', ' ', text) # \\d+ => các ký tự là chứ số d = [0123456789] => d+ 20 33 3 9999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4f2d156-59e4-4c3c-a266-f768aa33c1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.\n",
    "def del_space(doc):\n",
    "    space_pattern = r'\\s+' # \\s => khoảng trắng (dấu cách, dấu tab, xuống dòng) trừ dấu cách đứng 1 mình => từ 1 khoảng trắng \\s+\n",
    "    return re.sub(space_pattern, ' ', doc.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c281743-b3c3-4119-a0da-e99c8cc988fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sáng ngày 9/7, Hội đồng Tư vấn chính sách tài chính, tiền tệ quốc gia đã họp dưới sự chủ trì của Thủ\n",
      "del_html : Sáng ngày 9/7, Hội đồng Tư vấn chính sách tài chính, tiền tệ quốc gia đã họp dưới sự chủ trì của Thủ\n",
      "del_link : Sáng ngày 9/7, Hội đồng Tư vấn chính sách tài chính, tiền tệ quốc gia đã họp dưới sự chủ trì của Thủ\n",
      "del_emoji : Sáng ngày 9/7, Hội đồng Tư vấn chính sách tài chính, tiền tệ quốc gia đã họp dưới sự chủ trì của Thủ\n",
      "del_punctuation : Sáng ngày 9 7  Hội đồng Tư vấn chính sách tài chính  tiền tệ quốc gia đã họp dưới sự chủ trì của Thủ\n",
      "del_numbers : Sáng ngày      Hội đồng Tư vấn chính sách tài chính  tiền tệ quốc gia đã họp dưới sự chủ trì của Thủ\n",
      "del_space : sáng ngày hội đồng tư vấn chính sách tài chính tiền tệ quốc gia đã họp dưới sự chủ trì của thủ\n"
     ]
    }
   ],
   "source": [
    "text = data.iloc[0, 2][:100]\n",
    "print(text)\n",
    "for func in [del_html, del_link, del_emoji, del_punctuation, \n",
    "             del_numbers, del_space]:\n",
    "    text = func(text)\n",
    "    print(func.__name__, ':', text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16c807e-c950-44b4-923a-3da9c08f74e8",
   "metadata": {},
   "source": [
    "#### Chia từ (Word tokenizing) - Tiếng Anh vs Tiếng Việt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c985a9ba-9a19-4e02-9b09-e99c24578207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tiếng Anh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "837abb62-743b-4a2d-9c54-54098e33f6a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sáng', 'ngày', 'hội', 'đồng', 'tư', 'vấn', 'chính', 'sách', 'tài', 'chính', 'tiền', 'tệ', 'quốc', 'gia', 'đã', 'họp', 'dưới', 'sự', 'chủ', 'trì', 'của', 'thủ']\n"
     ]
    }
   ],
   "source": [
    "print(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3377d6c5-7b47-4a12-9dd4-9b5385f07928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\tdoan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (3.6.2)\n",
      "Requirement already satisfied: regex in c:\\users\\tdoan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from nltk) (2021.8.21)\n",
      "Requirement already satisfied: tqdm in c:\\users\\tdoan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from nltk) (4.62.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\tdoan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: click in c:\\users\\tdoan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from nltk) (8.0.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\tdoan\\appdata\\roaming\\python\\python38\\site-packages (from click->nltk) (0.4.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "734b902e-accd-4b91-84ae-86700ca78c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e7421b75-c2cf-407c-8506-a1ba5a31d44b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', 'quick', 'brown', 'fox', 'jump', 'over', 'a', 'lazy', 'dog', '.']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = 'A quick brown fox jump over a lazy dog.'\n",
    "nltk.word_tokenize(sentence) # tách thành yếu tổ nhỏ nhất trong câu (word & token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0eda43ab-e1ba-4619-be81-04e8a1e1f738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', 'quick', 'brown', 'fox', 'jump', 'over', 'a', 'lazy', 'dog.']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence.split() # tách ở dấu cách"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "08e1184d-1a5f-4ec0-8621-cc3035cd3e59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A quick brown fox jump over a lazy dog.']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.sent_tokenize(sentence) # setence tokenize # tách câu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b1e32561-efc2-4f71-9643-e2812fc0fcb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['A', 'quick', 'brown', 'fox', 'jump', 'over', 'a', 'lazy', 'dog', '.'],\n",
       " ['He', 'is', 'the', 'royal', 'king', '.']]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = '''A quick brown fox jump over a lazy dog.\n",
    "He is the royal king.\n",
    "'''\n",
    "\n",
    "[nltk.word_tokenize(i) for i in nltk.sent_tokenize(sentences)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6872852e-1348-416d-a270-90b0eb348b9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['A', 'quick', 'brown', 'fox', 'jump', 'over', 'a', 'lazy', 'dog'],\n",
       " ['He', 'is', 'the', 'royal', 'king'],\n",
       " []]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[sent.split() for sent in sentences.split('.')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a27ba149-2dfd-409f-a23f-550018530fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sáng', 'ngày', 'hội', 'đồng', 'tư', 'vấn', 'chính', 'sách', 'tài', 'chính', 'tiền', 'tệ', 'quốc', 'gia', 'đã', 'họp', 'dưới', 'sự', 'chủ', 'trì', 'của', 'thủ']\n"
     ]
    }
   ],
   "source": [
    "print(nltk.word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ea9033be-796a-42a6-a4ab-7fd5764c5186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tiếng Việt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f6fbfac9-9bab-4af3-b671-b64aad2476bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyvi in c:\\users\\tdoan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (0.1.1)\n",
      "Requirement already satisfied: sklearn-crfsuite in c:\\users\\tdoan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pyvi) (0.3.6)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\tdoan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pyvi) (0.24.2)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\tdoan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scikit-learn->pyvi) (1.7.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\tdoan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scikit-learn->pyvi) (2.1.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\tdoan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scikit-learn->pyvi) (1.19.5)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\tdoan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scikit-learn->pyvi) (1.0.1)\n",
      "Requirement already satisfied: tqdm>=2.0 in c:\\users\\tdoan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from sklearn-crfsuite->pyvi) (4.62.1)\n",
      "Requirement already satisfied: tabulate in c:\\users\\tdoan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from sklearn-crfsuite->pyvi) (0.8.9)\n",
      "Requirement already satisfied: six in c:\\users\\tdoan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from sklearn-crfsuite->pyvi) (1.15.0)\n",
      "Requirement already satisfied: python-crfsuite>=0.8.3 in c:\\users\\tdoan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from sklearn-crfsuite->pyvi) (0.9.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\tdoan\\appdata\\roaming\\python\\python38\\site-packages (from tqdm>=2.0->sklearn-crfsuite->pyvi) (0.4.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyvi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fd019cd1-8530-4639-8868-5159e79cd2b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sáng ngày hội_đồng tư_vấn chính_sách tài_chính tiền_tệ quốc_gia đã họp dưới sự chủ_trì của thủ'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyvi.ViTokenizer import tokenize\n",
    "\n",
    "tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5bd0bb65-92a9-429a-8f4c-8c6c074f8b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sáng', 'ngày', 'hội_đồng', 'tư_vấn', 'chính_sách', 'tài_chính', 'tiền_tệ', 'quốc_gia', 'đã', 'họp', 'dưới', 'sự', 'chủ_trì', 'của', 'thủ']\n"
     ]
    }
   ],
   "source": [
    "print(tokenize(text).split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec9ba76-913e-4803-a39b-d6b537ed57bb",
   "metadata": {},
   "source": [
    "#### Put together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c038b40f-907a-4cea-bd75-9ff06581b9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = del_html(text)\n",
    "    text = del_link(text)\n",
    "    text = del_numbers(text)\n",
    "    text = del_emoji(text)\n",
    "    text = del_punctuation(text)\n",
    "    text = del_space(text)\n",
    "    return tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "98d08d23-2ebd-44e8-be76-6b7866777708",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      Thủ tướng: Khác với đa số các nước, dư địa chí...\n",
       "1      Thống nhất kịch bản tăng trưởng 3-4%, lạm phát...\n",
       "2      Đề xuất tăng liều lượng gói kích thích kinh tế...\n",
       "3      Thủ tướng: Khác với đa số các nước, dư địa chí...\n",
       "4      Tác động từ chính sách tín dụng: BĐS có thể đó...\n",
       "                             ...                        \n",
       "871    Việt Nam thành lập Viện nghiên cứu phát triển ...\n",
       "872    Bất động sản chờ lực bật mới Chịu nhiều khó kh...\n",
       "873    Diễn đàn Bất động sản 2020: Cơ hội mới từ chín...\n",
       "874    Thống đốc Lê Minh Hưng: Tín dụng tăng trở lại,...\n",
       "875    Sẵn sàng đón nhận dòng vốn đầu tư dịch chuyển ...\n",
       "Name: corpus, Length: 876, dtype: object"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['corpus'] = data.apply(lambda x: ' '.join(x), axis=1) # gộp dữ liệu text từ 3 cột của dataframe\n",
    "data['corpus'] # text-mining tập hợp các văn bản text đc gọi là corpus/corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "664ac7b1-82d1-400c-bb94-0746b098a2ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      thủ_tướng khác với đa_số các nước dư địa_chính...\n",
       "1      thống_nhất kịch_bản tăng_trưởng lạm_phát dưới ...\n",
       "2      đề_xuất tăng liều_lượng gói kích_thích kinh_tế...\n",
       "3      thủ_tướng khác với đa_số các nước dư địa_chính...\n",
       "4      tác_động từ chính_sách tín_dụng bđs có_thể đón...\n",
       "                             ...                        \n",
       "871    việt nam thành_lập viện nghiên_cứu phát_triển ...\n",
       "872    bất_động_sản chờ lực bật mới chịu nhiều khó_kh...\n",
       "873    diễn_đàn bất_động_sản cơ_hội mới từ chính_sách...\n",
       "874    thống_đốc lê minh hưng tín_dụng tăng trở_lại n...\n",
       "875    sẵn_sàng đón_nhận dòng vốn đầu_tư dịch_chuyển ...\n",
       "Name: corpus, Length: 876, dtype: object"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['corpus'] = data['corpus'].apply(clean_text)\n",
    "data['corpus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "df990f68-245c-41ed-8530-3c92e0947699",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'thủ_tướng khác với đa_số các nước dư địa_chính_sách tài khóa tiền_tệ của việt nam còn khá lớn tổ_quốc với bối_cảnh hiện_nay hội_đồng tư_vấn chính_sách tài_chính tiền_tệ quốc_gia thống_nhất kịch_bản tăng_trưởng từ kiểm_soát lạm_phát dưới năm và đầu tăng_trưởng tín_dụng trên chủ_trương tăng thêm bội_chi ngân_sách nợ công khoảng gdp sáng ngày hội_đồng tư_vấn chính_sách tài_chính tiền_tệ quốc_gia đã họp dưới sự chủ_trì của thủ_tướng nguyễn xuân phúc chủ_tịch hội_đồng dư địa_chính_sách tài khóa tiền_'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[0, 'corpus'][:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa513f1e-0125-4cdc-a268-883af940025b",
   "metadata": {},
   "source": [
    "### Mã hóa dữ liệu text (Word embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d782c1-6302-4a8e-b8de-e936565beb95",
   "metadata": {},
   "source": [
    "#### One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "76b015ad-aa33-49a1-9937-677dffac05fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chuyển thành dạng lowercase (nếu chưa)\n",
    "# tách từ (tokenize), loại bỏ từ trùng lặp (set)\n",
    "# Sắp xếp theo thứ tự a-z\n",
    "# Lấy giá trị vị trí\n",
    "# Chuyển đổi thành vector [0, 1] tương ứng giá trị vị trí"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8e93876c-d23f-4e3b-b6f0-8e21ee77c6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4]\n",
      "['can', 'eat', 'i', 'pizza', 'the']\n"
     ]
    }
   ],
   "source": [
    "sentence = 'Can I eat the pizza'\n",
    "arr = sorted(nltk.word_tokenize(sentence.lower()))\n",
    "print(list(range(len(arr))))\n",
    "print(arr)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "97cd81d8-3d7c-4aa6-86b9-45cdfa7eb8e1",
   "metadata": {},
   "source": [
    "[[1. 0. 0. 0. 0.] # can\n",
    " [0. 0. 1. 0. 0.] # i\n",
    " [0. 1. 0. 0. 0.] # eat\n",
    " [0. 0. 0. 0. 1.] # the\n",
    " [0. 0. 0. 1. 0.]] # pizza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b38888df-bbca-4618-9546-b49bc1be16e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "values:  ['can', 'eat', 'i', 'pizza', 'the']\n",
      "\n",
      "integer encoded:  [0, 2, 1, 4, 3]\n",
      "\n",
      "MATRIX:\n",
      "[[1 0 0 0 0]\n",
      " [0 0 1 0 0]\n",
      " [0 1 0 0 0]\n",
      " [0 0 0 0 1]\n",
      " [0 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "docs = \"Can I eat the Pizza\".lower().split()\n",
    "doc1 = set(docs)\n",
    "doc1 = sorted(doc1)\n",
    "print (\"\\nvalues: \", doc1)\n",
    "\n",
    "integer_encoded = []\n",
    "for i in docs:\n",
    "    v = np.where( np.array(doc1) == i)[0][0]\n",
    "    integer_encoded.append(v)\n",
    "print (\"\\ninteger encoded: \",integer_encoded)\n",
    "\n",
    "def get_vec(len_doc,word):\n",
    "    empty_vector = [0] * len_doc\n",
    "    vect = 0\n",
    "    find = np.where( np.array(doc1) == word)[0][0]\n",
    "    empty_vector[find] = 1\n",
    "    return empty_vector\n",
    "\n",
    "def get_matrix(doc1):\n",
    "    mat = []\n",
    "    len_doc = len(doc1)\n",
    "    for i in docs:\n",
    "        vec = get_vec(len_doc,i)\n",
    "        mat.append(vec)\n",
    "        \n",
    "    return np.asarray(mat)\n",
    "\n",
    "print (\"\\nMATRIX:\")\n",
    "print (get_matrix(doc1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b347065-59af-414d-8fee-f37389be5b5d",
   "metadata": {},
   "source": [
    "### Khai phá dữ liệu text (Text Mining)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b82a86e-d2e8-4c4c-a418-124c90218d1f",
   "metadata": {},
   "source": [
    "#### Khái niệm Bag-of-Words\n",
    "\n",
    "Là kỹ thuật chia văn bản thành các tổ hợp từ khác nhau (bằng phương pháp tokenize). Cách chia phổ biến là mỗi câu thành 1 văn bản (bag) và mỗi văn bản được chia thành từ (word). Dựa vào đó có thể đo lường mức độ xuất hiện của các từ trong văn bản và xây dựng mối liên hệ giữa ngữ cảnh và các từ. \n",
    "\n",
    "Hai yếu tố:\n",
    "1. Từ điển của các từ được sử dụng\n",
    "2. Mức độ xuất hiện của các từ trong từ điển\n",
    "*Mỗi từ hay token được gọi là một `gram`*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a2f157ea-c709-404b-a67c-fb87442d5f93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It was the best of times',\n",
       " 'It was the worst of times',\n",
       " 'It was the age of wisdom',\n",
       " 'It was the age of foolishness']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [\n",
    "    'It was the best of times', # best times\n",
    "    'It was the worst of times', # worst times\n",
    "    'It was the age of wisdom', # age wisdom\n",
    "    'It was the age of foolishness' # age foolishness\n",
    "]\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9c0142af-d81d-4d67-be62-1139799175e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 0, 1, 1, 1, 0, 0, 0]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "['It', 'was', 'the', 'best', 'of', 'times', 'worst', 'age', 'wisdom', 'foolishness'] # Vector từ điển\n",
    "[1, 1, 1, 1, 1, 1, 0, 0, 0, 0]\n",
    "[1, 1, 1, 0, 1, 1, 1, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dbea88b0-c557-4e32-92bf-d9ff4d024937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It', 'was', 'the', 'best', 'of', 'times', 'worst', 'age', 'wisdom', 'foolishness']\n"
     ]
    }
   ],
   "source": [
    "dictionary = [] # thêm dần vào từ điển\n",
    "for sent in sentences:\n",
    "    words = nltk.word_tokenize(sent) # tokenize\n",
    "    for w in words:\n",
    "        if w not in dictionary:\n",
    "            dictionary.append(w)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "125e3c08-234f-4810-a497-c50e7c531ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was the best of times :\t [1, 1, 1, 1, 1, 1, 0, 0, 0, 0]\n",
      "It was the worst of times :\t [1, 1, 1, 0, 1, 1, 1, 0, 0, 0]\n",
      "It was the age of wisdom :\t [1, 1, 1, 0, 1, 0, 0, 1, 1, 0]\n",
      "It was the age of foolishness :\t [1, 1, 1, 0, 1, 0, 0, 1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "for sent in sentences:\n",
    "    vec = [1 if w in sent else 0 for w in dictionary]\n",
    "    print(sent, ':\\t', vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3645f46c-63e0-4a38-a1f5-ad8ab78b8aa3",
   "metadata": {},
   "source": [
    "#### TF-IDF (Term frequency-Inverse Document frequency)\n",
    "\n",
    "Ngữ cảnh: \"I am very angry\" ==> \"very angry\" # tập trung vào các từ mang nhiều thông tin\n",
    "\n",
    "Đơn vị để đo thông tin trong khoa học machine-learning: entropy\n",
    "\n",
    "Đo lường tần suất ***hợp lý*** một từ (hay token) xuất hiện trong văn bản. Tần suất này được tính bằng: Mức độ xuất hiện của từ trong văn bản chia cho tỉ lệ văn bản mà từ đó xuất hiện trên tổng tất cả số lượng văn bản.\n",
    "\n",
    "Ví dụ: đối với như `what` hay `the`, các từ này xuất hiện rất nhiều tuy nhiên ko mang nhiều ý nghĩa nên cần có phương pháp loại trừ các từ này ra khỏi mô hình. Vì vậy ngoài tính toán mức độ xuất hiện của các từ trong văn bản, tuy nhiên nếu văn bản nào cũng xuất hiện từ này (hoặc đơn giản là rất nhiều > 90%) thì các từ này sẽ bị loại ra.\n",
    "\n",
    "$$ tf-idf(t, d, D)  = tf(t, d) \\dot idf(t, D)$$\n",
    "\n",
    "> `t`: từ (word hay token)\n",
    " \n",
    "> `d`: văn bản (document)\n",
    " \n",
    "> `D`: tệp các văn bản (documents)\n",
    "\n",
    "trong đó:\n",
    "\n",
    "$$ tf(t, d) = \\log(1 + freq(t, d)) $$\n",
    "$$ idf(t, D) = \\log \\left( \\dfrac{N}{count(d \\in D: t \\in d)} \\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8d500e30-be68-45fe-8fcb-296cc4c460cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\tdoan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (4.0.1)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\users\\tdoan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from gensim) (1.19.5)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\tdoan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from gensim) (1.7.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\tdoan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from gensim) (5.2.0)\n",
      "Requirement already satisfied: Cython==0.29.21 in c:\\users\\tdoan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from gensim) (0.29.21)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "97fd43b9-a214-460b-96a2-a7e525f0df43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\tdoan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora, models\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8b11b6e9-3271-4c0b-b9b8-51709f57ff3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['it', 'was', 'the', 'best', 'of', 'times'],\n",
       " ['it', 'was', 'the', 'worst', 'of', 'times'],\n",
       " ['it', 'was', 'the', 'age', 'of', 'wisdom'],\n",
       " ['it', 'was', 'the', 'age', 'of', 'foolishness']]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized = [simple_preprocess(sent) for sent in sentences] # preprocessing đơn giản\n",
    "tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bb319f69-70ac-4e82-ac01-ded493dc4e53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1)],\n",
       " [(1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1)],\n",
       " [(1, 1), (2, 1), (3, 1), (5, 1), (7, 1), (8, 1)],\n",
       " [(1, 1), (2, 1), (3, 1), (5, 1), (7, 1), (9, 1)]]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary() # Xây dựng từ điển\n",
    "bow_corpus = [dictionary.doc2bow(doc, allow_update=True) for doc in tokenized] # Tạo Bag-of-Word\n",
    "bow_corpus # Bao gồm các cặp id và count của từng từ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4d37112c-a377-4e54-a479-749d95fb1d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['best', 1], ['it', 1], ['of', 1], ['the', 1], ['times', 1], ['was', 1]]\n",
      "[['it', 1], ['of', 1], ['the', 1], ['times', 1], ['was', 1], ['worst', 1]]\n",
      "[['it', 1], ['of', 1], ['the', 1], ['was', 1], ['age', 1], ['wisdom', 1]]\n",
      "[['it', 1], ['of', 1], ['the', 1], ['was', 1], ['age', 1], ['foolishness', 1]]\n"
     ]
    }
   ],
   "source": [
    "for doc in bow_corpus:\n",
    "    print([[dictionary[id], freq] for id, freq in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c84a2c4c-d198-4445-8b01-9b12c23990f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.tfidfmodel.TfidfModel at 0x1d99e145130>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = models.TfidfModel(bow_corpus, smartirs='ntc')\n",
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "25d55915-d15d-46b4-98a7-878a8d61118a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['best', 0.8448462391634638], ['it', 0.11713529839512135], ['of', 0.11713529839512135], ['the', 0.11713529839512135], ['times', 0.48099076877929264], ['was', 0.11713529839512135]]\n",
      "[['it', 0.11713529839512132], ['of', 0.11713529839512132], ['the', 0.11713529839512132], ['times', 0.48099076877929253], ['was', 0.11713529839512132], ['worst', 0.8448462391634637]]\n",
      "[['it', 0.11713529839512132], ['of', 0.11713529839512132], ['the', 0.11713529839512132], ['was', 0.11713529839512132], ['age', 0.48099076877929253], ['wisdom', 0.8448462391634637]]\n",
      "[['it', 0.11713529839512132], ['of', 0.11713529839512132], ['the', 0.11713529839512132], ['was', 0.11713529839512132], ['age', 0.48099076877929253], ['foolishness', 0.8448462391634637]]\n"
     ]
    }
   ],
   "source": [
    "for doc in tfidf[bow_corpus]:\n",
    "    print([[dictionary[id], freq] for id, freq in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ada12a94-82d2-4973-8041-1e8583a55584",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['biện_pháp', 0.041056865547833686],\n",
       " ['bán_lẻ', 0.04908556240345798],\n",
       " ['bên', 0.022150442731134225],\n",
       " ['bùi', 0.07118245725073921],\n",
       " ['bùng_phát', 0.04460812603722285],\n",
       " ['bảo_vệ', 0.04995756609294067],\n",
       " ['bối_cảnh', 0.034662574749689465],\n",
       " ['bội_chi', 0.1006266484130534],\n",
       " ['ca', 0.060608811632763285],\n",
       " ['chi', 0.04908556240345798]]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary()\n",
    "doc_tokenized = [doc.split(' ') for doc in data['corpus']] # corpus tiếng việt\n",
    "BoW_corpus = [dictionary.doc2bow(doc, allow_update=True) for doc in doc_tokenized]\n",
    "\n",
    "model = models.TfidfModel(BoW_corpus)\n",
    "[[dictionary[id], freq] for id, freq in model[BoW_corpus[0]][:10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bc2036-7dc6-4b33-baac-7c1ab14fe285",
   "metadata": {},
   "source": [
    "#### Mô hình Word2vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966f5ac5-3024-4991-8a7b-27ec8002ac29",
   "metadata": {},
   "source": [
    "*\"Word2Vec was developed at Google by Tomas Mikolov, et al. and uses Neural Networks to learn word embeddings. The beauty with word2vec is that the vectors are learned by understanding the context in which words appear. The result is vectors in which words with similar meanings end up with a similar numerical representation.\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe00fd3-483f-4343-b331-8bdbde4d8d03",
   "metadata": {},
   "source": [
    "**One-hot-encoding**\n",
    "\n",
    "All word are treated equal\n",
    "\n",
    "![](https://i2.wp.com/insightbot.blob.core.windows.net/blogimagecontainer/b3c56245-db43-48ab-b652-9ba03f4d9900.jpg?ssl=1)\n",
    "\n",
    "**Word2Vec**\n",
    "\n",
    "Word with similar numeric value are similar in meaning\n",
    "\n",
    "![](https://i1.wp.com/insightbot.blob.core.windows.net/blogimagecontainer/8cbfc874-3ba3-46c8-ab68-2711812ecbf1.jpg?ssl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124e7484-2125-4bfa-8033-9f11341f6c93",
   "metadata": {},
   "source": [
    "Hai loại mô hình Word2Vec: **CBOW** (Continuous Bag-of-Word) và **Skip-Gram**\n",
    "\n",
    "Continuous Bag of Words (CBOW): *nhìn hình (ngữ cảnh) đoán chữ*\n",
    "\n",
    "Ngược lại, Skip-Gram: *nhìn chữ đoán hình (ngữ cảnh)*\n",
    "\n",
    "![](https://i0.wp.com/insightbot.blob.core.windows.net/blogimagecontainer/7938152f-71c8-4f28-9c25-06735e6e2b67.jpg?ssl=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1e4d21d0-2f49-4f45-a365-5f0c2718df94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "41c90846-6e81-448f-b604-c797cab58b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = Word2Vec(doc_tokenized, min_count=5, sg=0) # 1 == skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "58d845b9-f5aa-4e68-b443-7e786b4bbc7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thủ_tướng', 'khác', 'với', 'đa_số', 'các', 'nước', 'dư', 'địa_chính_sách', 'tài', 'khóa', 'tiền_tệ', 'của', 'việt', 'nam', 'còn', 'khá', 'lớn', 'tổ_quốc', 'với', 'bối_cảnh', 'hiện_nay', 'hội_đồng', 'tư_vấn', 'chính_sách', 'tài_chính', 'tiền_tệ', 'quốc_gia', 'thống_nhất', 'kịch_bản', 'tăng_trưởng', 'từ', 'kiểm_soát', 'lạm_phát', 'dưới', 'năm', 'và', 'đầu', 'tăng_trưởng', 'tín_dụng', 'trên', 'chủ_trương', 'tăng', 'thêm', 'bội_chi', 'ngân_sách', 'nợ', 'công', 'khoảng', 'gdp', 'sáng', 'ngày', 'hội_đồng', 'tư_vấn', 'chính_sách', 'tài_chính', 'tiền_tệ', 'quốc_gia', 'đã', 'họp', 'dưới', 'sự', 'chủ_trì', 'của', 'thủ_tướng', 'nguyễn', 'xuân', 'phúc', 'chủ_tịch', 'hội_đồng', 'dư', 'địa_chính_sách', 'tài', 'khóa', 'tiền_tệ', 'còn', 'lớn', 'thủ_tướng', 'nguyễn', 'xuân', 'phúc', 'cho', 'rằng', 'dịch_bệnh', 'covid', 'bùng_phát', 'diện', 'rộng', 'trên', 'toàn_cầu', 'diễn_biến', 'phức_tạp', 'chưa', 'dừng', 'lại', 'nhất_là', 'tại', 'các', 'đối_tác', 'lớn', 'của']\n"
     ]
    }
   ],
   "source": [
    "print(doc_tokenized[0][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e0289002-5b51-4884-8bb6-8cbaf18f8cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "các                  0\n",
      "và                   1\n",
      "trong                2\n",
      "kinh_tế              3\n",
      "của                  4\n",
      "                  ... \n",
      "ðồng              3389\n",
      "macroeconomics    3390\n",
      "bốc_hơi           3391\n",
      "coi_trọng         3392\n",
      "xói_mòn           3393\n",
      "Length: 3394, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "vocabulary = word2vec.wv.key_to_index\n",
    "print(pd.Series(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f4bdfb20-169b-46c0-833f-0c3e1859d1b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.9037295 ,  0.49264824, -0.75975233, -1.7381631 , -0.8911547 ,\n",
       "       -0.82742673,  0.46276957,  0.16021186,  0.3758612 ,  1.191963  ,\n",
       "        0.34713018, -1.7967975 ,  1.732466  ,  0.5577671 ,  0.80041695,\n",
       "        1.6470114 , -1.4448764 , -1.7786255 ,  1.4733684 , -2.5320716 ,\n",
       "        0.6664354 ,  0.4117893 ,  3.3007782 , -0.30072892, -0.94906694,\n",
       "       -0.4649296 , -0.51784086,  1.2303107 , -0.17780925,  1.1169204 ,\n",
       "       -0.7867368 ,  0.32211864, -1.1892025 , -0.42338714, -0.01200487,\n",
       "        0.19655043,  0.7250242 ,  0.6220518 , -0.6798909 , -1.6635782 ,\n",
       "       -0.13375671,  0.44558758, -0.4267363 , -0.0852995 ,  1.3760791 ,\n",
       "        1.0069851 , -0.27673408,  0.92233634,  1.3963648 , -0.90076655,\n",
       "        1.5796556 ,  0.3899514 , -2.2636557 , -1.0684747 ,  1.8599749 ,\n",
       "       -1.0085125 ,  0.35641035, -0.17802429,  0.18548755,  0.2209606 ,\n",
       "       -0.6355668 , -0.24265128, -2.1131837 ,  1.924271  , -0.87514234,\n",
       "        0.911427  , -1.6011363 , -0.24120401,  0.91147625,  1.6490867 ,\n",
       "       -0.5658606 , -0.24965614, -1.1164724 ,  1.8973159 , -0.56314737,\n",
       "        1.5844469 , -0.2411963 , -0.76126623, -0.01447778,  2.7428467 ,\n",
       "       -0.72242486, -0.5977616 ,  0.68614525,  1.2785829 ,  0.80003846,\n",
       "        0.05587743,  0.10248137,  1.9227101 ,  1.0595695 ,  0.3525892 ,\n",
       "        1.347993  ,  0.71424913,  1.1771193 , -0.49092114, -0.35660842,\n",
       "        0.9253629 ,  0.8681362 , -0.3031483 ,  0.47219977,  0.5457087 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1 = word2vec.wv['bùng_phát']\n",
    "v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "245edc78-bd0b-4642-b508-20c65552ea40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lây_nhiễm', 0.7020787596702576),\n",
       " ('quay', 0.6423612236976624),\n",
       " ('làn_sóng', 0.6407276391983032),\n",
       " ('hậu_quả', 0.6226053237915039),\n",
       " ('ổ', 0.5950023531913757),\n",
       " ('coronavirus', 0.5911593437194824),\n",
       " ('lây_lan', 0.5906933546066284),\n",
       " ('ngăn_chặn', 0.5804805755615234),\n",
       " ('brexit', 0.5669182538986206),\n",
       " ('tái_phát', 0.5645425319671631)]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_words = word2vec.wv.most_similar('bùng_phát')\n",
    "sim_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03aea233-3d03-4566-8876-153325d71cd9",
   "metadata": {},
   "source": [
    "**Skip-gram**\n",
    "\n",
    "Window Size defines how many words before and after the target word will be used as context, typically a Window Size is 5. \n",
    "![](https://i2.wp.com/insightbot.blob.core.windows.net/blogimagecontainer/a8066c1d-c532-4549-bb24-19dfea5eb178_med.jpg?ssl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860004a0-d665-4d69-a4b6-41f7ef94ccea",
   "metadata": {},
   "source": [
    "Using a window size of 2 the input pairs for training on w(4) royal would be:\n",
    "![](https://israelg99.github.io/images/2017-03-23-Word2Vec-Explained/training_data.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f63ee0db-98c0-4baa-84b4-9a08b988bfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = Word2Vec(doc_tokenized, min_count=5, sg=1) # 1 == skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d07e184d-7c60-4ed9-9f52-11fb709641d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "các                  0\n",
      "và                   1\n",
      "trong                2\n",
      "kinh_tế              3\n",
      "của                  4\n",
      "                  ... \n",
      "ðồng              3389\n",
      "macroeconomics    3390\n",
      "bốc_hơi           3391\n",
      "coi_trọng         3392\n",
      "xói_mòn           3393\n",
      "Length: 3394, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "vocabulary = word2vec.wv.key_to_index\n",
    "print(pd.Series(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "63630684-fb57-41cc-bf89-ccb47f16e5a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.2704635 ,  0.03681991, -0.11385291, -0.19227746, -0.27433053,\n",
       "       -1.013911  , -0.02834329,  0.1007681 ,  0.44169915, -0.05684225,\n",
       "        0.26560184, -0.9090996 ,  0.1143927 , -0.04509869,  0.11894494,\n",
       "        0.4482545 , -0.68796974, -0.52933836,  0.41521764, -1.1534393 ,\n",
       "        0.49324867,  0.43718013,  0.96965677, -0.40331557, -0.34092933,\n",
       "        0.12640269,  0.55570734,  0.48846662, -0.52204525,  0.6077707 ,\n",
       "       -0.22835906,  0.0313529 ,  0.07060026, -0.33686653, -0.05852371,\n",
       "        0.13328752,  0.36528537,  0.7604441 , -0.04991452, -1.2114568 ,\n",
       "       -0.09419811, -0.06944511, -0.09239692, -0.10894652,  0.9145663 ,\n",
       "        0.2871258 , -0.11951722,  0.20946056,  0.5385248 ,  0.24145788,\n",
       "       -0.03153882,  0.16077532, -0.34992877, -0.36586374,  0.62914795,\n",
       "       -0.27384183, -0.33194116, -0.3251486 , -0.88937676,  0.99444103,\n",
       "       -0.01759175, -0.52954847, -0.83024555,  0.43601316, -0.22089948,\n",
       "        0.5574407 , -0.41052175, -0.22389777, -0.09237476,  0.33542752,\n",
       "        0.12625861, -0.91025174, -0.23891135,  0.24760865,  0.564724  ,\n",
       "        0.5998304 , -0.3550726 , -0.46241853,  0.02487809,  0.42693993,\n",
       "       -0.04949055, -0.0648625 , -0.81110173,  0.22651379, -0.13962494,\n",
       "        0.15828297,  0.32486242,  0.78024304,  0.05576845, -0.20013852,\n",
       "        0.75819373, -0.12953861, -0.47133192, -0.60413605,  0.12856904,\n",
       "        0.3605988 ,  0.29413518,  0.13681105,  0.01592259,  0.24074127],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1 = word2vec.wv['bùng_phát']\n",
    "v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "75f1823d-a501-4b76-87c3-45e174b30ebc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lây_nhiễm', 0.662135124206543),\n",
       " ('chiến_đấu', 0.6452938318252563),\n",
       " ('điều_trị', 0.6356167197227478),\n",
       " ('coronavirus', 0.6348608136177063),\n",
       " ('tái_phát', 0.5959097743034363),\n",
       " ('diện', 0.5861860513687134),\n",
       " ('chẳng', 0.5739536285400391),\n",
       " ('bất_ổn_định', 0.5678112506866455),\n",
       " ('làn_sóng', 0.5667248964309692),\n",
       " ('hậu_quả', 0.5660402178764343)]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_words = word2vec.wv.most_similar('bùng_phát')\n",
    "sim_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c9161eac-d4a4-499f-a291-150bb9bbdab7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lây_nhiễm', 0.7020787596702576),\n",
       " ('quay', 0.6423612236976624),\n",
       " ('làn_sóng', 0.6407276391983032),\n",
       " ('hậu_quả', 0.6226053237915039),\n",
       " ('ổ', 0.5950023531913757),\n",
       " ('coronavirus', 0.5911593437194824),\n",
       " ('lây_lan', 0.5906933546066284),\n",
       " ('ngăn_chặn', 0.5804805755615234),\n",
       " ('brexit', 0.5669182538986206),\n",
       " ('tái_phát', 0.5645425319671631)]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[('lây_nhiễm', 0.7020787596702576),\n",
    " ('quay', 0.6423612236976624),\n",
    " ('làn_sóng', 0.6407276391983032),\n",
    " ('hậu_quả', 0.6226053237915039),\n",
    " ('ổ', 0.5950023531913757),\n",
    " ('coronavirus', 0.5911593437194824),\n",
    " ('lây_lan', 0.5906933546066284),\n",
    " ('ngăn_chặn', 0.5804805755615234),\n",
    " ('brexit', 0.5669182538986206),\n",
    " ('tái_phát', 0.5645425319671631)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f317fa05-a16e-4d38-acac-20452e86add7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
