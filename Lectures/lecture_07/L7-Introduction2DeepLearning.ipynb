{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./image/deep_learning.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Introduction to Deep Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/proxy/1*4wREvShuhT8kxxPFpKBd6A.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Brief Content of this Notebook:**\n",
    ">* Perceptron Learning Algorithm\n",
    "* Multi-Layer Perceptron and Backpropagation\n",
    "* Training an MLP with Tensorflow  \n",
    "* Fine-Tuning Neural Network Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **[Part 1: Perceptron Learning Algorithm](#Part-1:-Perceptron-Learning-Algorithm)**  \n",
    "\n",
    "\n",
    "* **[Part 2: Multi-Layer Perceptron and Backpropagation](#Part-2:-Multi-Layer-Perceptron-and-Backpropagation)**  \n",
    "\n",
    "\n",
    "* **[Part 3: Training an MLP with Tensorflow ](#Part-3:-Training-an-MLP-with-Tensorflow)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Perceptron Learning Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning, the perceptron is an algorithm for supervised learning of binary classifiers. A binary classifier is a function which can decide whether or not an input, represented by a vector of numbers, belongs to some specific class. The Perceptron is one of the simplest artificial neural networks (ANNs) architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets build the perceptron model with a simple classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/max/684/0*RuUpWDXIqL_tWpvH.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A perceptron consists of one or more inputs, a processor, and a single output. The inputs and output are numbers and each input connection is associated with a weight. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say, we have the input and output data\n",
    "* Input: \n",
    ">* x1 = Height of the person \n",
    ">* x2 = Weight of the person\n",
    "* Output:\n",
    ">* y = Gender(Male/Female)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/max/753/1*p2PTL_NRQA93-RDdnAGdvg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our motive is to fit a decision boundary(a line) that separates all the male samples from the female samples. We’ll use the perceptron model that’ll find the equation of the decision boundary for us. All we have to do is feed the input and output data for the model to train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/max/753/1*kjQhQv0R2Iw9bixtywW5rQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general equation of a straight line is $ax + by + c = 0$ or we can rephrase $w_0 + w_1x_1 + w_2x_2 = 0  \\quad \\quad (1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** \n",
    "1. If $P(x_P, y_P)$ lies on the line, $w_0 + w_1x_P + w_2x_P = 0$\n",
    "2. If $Q(x_Q, y_Q)$ lies above the line, $w_0 + w_1x_Q + w_2x_Q > 0$\n",
    "3. If $R(x_R, y_R)$ lies below the line, $w_0 + w_1x_R + w_2x_R < 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this intuition, we can classify any point by substituting its value in the line equation. If the resultant value is positive, the sample belongs to class Male $(Y = 1)$, if negative, the sample is a female sample $(Y = -1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/max/1001/1*3FFsnCSmKckphHshsRbH3Q.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On Plotting the above property discussed, we get a function called the Sign function. This is the activation function that we are going to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define equation $(1)$ as dot product of vectors $W$ and $X$ such as $X \\dot W = 0 \\quad (2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector $X$:\n",
    "![](https://miro.medium.com/max/429/1*Dusv9RkqITs34wAMNaEnvg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector $W$:\n",
    "![](https://miro.medium.com/max/305/1*w75nZrDOt_AwqkHWxBimUw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model correctly classifies the sample $X$ if $Y * (X \\dot W) > 0$. The sample is misclassified if $- Y * (X \\dot W) > 0$. We define a cost function for misclassified sample $X_i$:\n",
    "\n",
    "$$J(\\mathbf{W}; \\mathbf{X}_i; Y_i) = -Y_i\\mathbf{W}\\mathbf{X}_i$$\n",
    "\n",
    "We'll use an optimization algorithm, called the Gradient Descent to minimizes the cost function by gradually updating the weight values.\n",
    "\n",
    "Firstly, calculate the derivatives:\n",
    "\n",
    "$$\\nabla_{\\mathbf{W}}J(\\mathbf{W}; \\mathbf{X}_i; Y_i) = -Y_i\\mathbf{X}_i\n",
    "$$\n",
    "\n",
    "Update $W$:\n",
    "$$\\mathbf{W} = \\mathbf{W} + \\eta Y_i\\mathbf{X}_i\n",
    "$$\n",
    "with $\\eta$ is learning rate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/proxy/1*pPYFYOZ-8dja-kRrkmuGOQ.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0    1    2    3               4\n",
       "0    5.1  3.5  1.4  0.2     Iris-setosa\n",
       "1    4.9  3.0  1.4  0.2     Iris-setosa\n",
       "2    4.7  3.2  1.3  0.2     Iris-setosa\n",
       "3    4.6  3.1  1.5  0.2     Iris-setosa\n",
       "4    5.0  3.6  1.4  0.2     Iris-setosa\n",
       "..   ...  ...  ...  ...             ...\n",
       "145  6.7  3.0  5.2  2.3  Iris-virginica\n",
       "146  6.3  2.5  5.0  1.9  Iris-virginica\n",
       "147  6.5  3.0  5.2  2.0  Iris-virginica\n",
       "148  6.2  3.4  5.4  2.3  Iris-virginica\n",
       "149  5.9  3.0  5.1  1.8  Iris-virginica\n",
       "\n",
       "[150 rows x 5 columns]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', header=None)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\documents\\other\\mci\\scorecard\\venv\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "data = data[:100]\n",
    "data[4] = np.where(data.iloc[:, -1]=='Iris-setosa', 0, 1)\n",
    "data = np.asmatrix(data, dtype = 'float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(np.array(data[:50,0]), np.array(data[:50,2]), marker='o', label='setosa')\n",
    "plt.scatter(np.array(data[50:,0]), np.array(data[50:,2]), marker='o', label='versicolor')\n",
    "plt.xlabel('petal length')\n",
    "plt.ylabel('sepal length')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:        \n",
    "    def fit(self, features, labels, num_iter=10):\n",
    "        self.weights = np.zeros(shape=(1, features.shape[1] + 1))\n",
    "        self.misclassified_ = []\n",
    "        \n",
    "        for epoch in range(num_iter):\n",
    "            misclassified = False\n",
    "            for x, label in zip(features, labels):\n",
    "                x = np.insert(x, 0, 1)\n",
    "                y = np.dot(self.weights, x.transpose())\n",
    "                \n",
    "                target = 1.0 if (y > 0) else 0.0\n",
    "                \n",
    "                delta = label[0] - target\n",
    "                \n",
    "                if delta != 0:\n",
    "                    misclassified += 1\n",
    "                    self.weights += delta * x\n",
    "            \n",
    "            print('Epoch {}'.format(epoch))\n",
    "            \n",
    "            self.misclassified_.append(misclassified)\n",
    "        \n",
    "    def predict(self, features):\n",
    "        new_features = np.concatenate([np.ones((len(features), 1)), features], axis=1)\n",
    "        y_pred = new_features * self.weights.T\n",
    "        print(y_pred)\n",
    "        y_pred = np.asarray(y_pred.flatten())[0]\n",
    "        print(y_pred)\n",
    "        labels_pred = np.where(y_pred > 0, 1, 0)\n",
    "        return labels_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "features = data[:, :-1]\n",
    "labels = data[:, -1]\n",
    "\n",
    "num_iter = 10\n",
    "\n",
    "model = Perceptron()\n",
    "model.fit(features, labels, num_iter)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = np.arange(1, num_iter+1)\n",
    "plt.plot(epochs, model.misclassified_)\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('misclassified')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-11.49]\n",
      " [ -9.47]\n",
      " [-10.49]\n",
      " [ -8.98]\n",
      " [-11.74]\n",
      " [-11.26]\n",
      " [-10.36]\n",
      " [-10.5 ]\n",
      " [ -8.56]\n",
      " [ -9.53]\n",
      " [-12.02]\n",
      " [ -9.76]\n",
      " [ -9.58]\n",
      " [-10.59]\n",
      " [-15.1 ]\n",
      " [-14.43]\n",
      " [-13.34]\n",
      " [-11.27]\n",
      " [-11.45]\n",
      " [-11.83]\n",
      " [ -9.9 ]\n",
      " [-11.25]\n",
      " [-13.38]\n",
      " [ -8.55]\n",
      " [ -8.2 ]\n",
      " [ -8.54]\n",
      " [ -9.54]\n",
      " [-11.08]\n",
      " [-11.24]\n",
      " [ -8.93]\n",
      " [ -8.68]\n",
      " [-10.5 ]\n",
      " [-13.46]\n",
      " [-14.45]\n",
      " [ -9.53]\n",
      " [-11.34]\n",
      " [-12.45]\n",
      " [ -9.53]\n",
      " [ -9.44]\n",
      " [-10.61]\n",
      " [-11.68]\n",
      " [ -6.81]\n",
      " [-10.16]\n",
      " [ -9.46]\n",
      " [ -9.53]\n",
      " [ -9.14]\n",
      " [-11.53]\n",
      " [ -9.86]\n",
      " [-11.91]\n",
      " [-10.66]\n",
      " [  7.3 ]\n",
      " [  7.14]\n",
      " [  9.03]\n",
      " [  8.33]\n",
      " [  8.99]\n",
      " [  8.91]\n",
      " [  8.15]\n",
      " [  4.33]\n",
      " [  8.08]\n",
      " [  6.92]\n",
      " [  6.7 ]\n",
      " [  6.85]\n",
      " [  7.48]\n",
      " [  9.37]\n",
      " [  3.98]\n",
      " [  6.43]\n",
      " [  8.74]\n",
      " [  6.42]\n",
      " [ 10.96]\n",
      " [  6.54]\n",
      " [  9.91]\n",
      " [  5.87]\n",
      " [ 11.85]\n",
      " [  9.29]\n",
      " [  6.74]\n",
      " [  6.9 ]\n",
      " [  9.48]\n",
      " [ 10.57]\n",
      " [  8.66]\n",
      " [  3.77]\n",
      " [  6.49]\n",
      " [  5.75]\n",
      " [  5.82]\n",
      " [ 12.72]\n",
      " [  8.96]\n",
      " [  7.08]\n",
      " [  8.21]\n",
      " [  9.53]\n",
      " [  6.22]\n",
      " [  7.61]\n",
      " [  9.11]\n",
      " [  8.49]\n",
      " [  6.7 ]\n",
      " [  4.58]\n",
      " [  7.82]\n",
      " [  6.41]\n",
      " [  6.99]\n",
      " [  6.96]\n",
      " [  2.41]\n",
      " [  6.83]]\n",
      "[-11.49  -9.47 -10.49  -8.98 -11.74 -11.26 -10.36 -10.5   -8.56  -9.53\n",
      " -12.02  -9.76  -9.58 -10.59 -15.1  -14.43 -13.34 -11.27 -11.45 -11.83\n",
      "  -9.9  -11.25 -13.38  -8.55  -8.2   -8.54  -9.54 -11.08 -11.24  -8.93\n",
      "  -8.68 -10.5  -13.46 -14.45  -9.53 -11.34 -12.45  -9.53  -9.44 -10.61\n",
      " -11.68  -6.81 -10.16  -9.46  -9.53  -9.14 -11.53  -9.86 -11.91 -10.66\n",
      "   7.3    7.14   9.03   8.33   8.99   8.91   8.15   4.33   8.08   6.92\n",
      "   6.7    6.85   7.48   9.37   3.98   6.43   8.74   6.42  10.96   6.54\n",
      "   9.91   5.87  11.85   9.29   6.74   6.9    9.48  10.57   8.66   3.77\n",
      "   6.49   5.75   5.82  12.72   8.96   7.08   8.21   9.53   6.22   7.61\n",
      "   9.11   8.49   6.7    4.58   7.82   6.41   6.99   6.96   2.41   6.83]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(features)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Multi-Layer Perceptron and Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The perceptron is very useful for classifying data sets that are linearly separable. They encounter serious limitations with data sets that do not conform to this pattern. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://images.deepai.org/glossary-terms/multilayer-perceptron-1878889.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MultiLayer Perceptron (MLPs) breaks this restriction and classifies datasets which are not linearly separable.  They do this by using a more robust and complex architecture to learn regression and classification models for difficult datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/proxy/1*eloYEyFrblGHVZhU345PJw.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-layer Perceptron (MLP) is a supervised learning algorithm that learns a function $f(\\cdot): R^m \\rightarrow R^o$ by training on a dataset, where $m$ is the number of dimensions for input and $o$ is the number of dimensions for output. Given a set of features $X = {x_1, x_2, ..., x_m}$ and a target $y$, it can learn a non-linear function approximator for either classification or regression. It is different from logistic regression, in that between the input and the output layer, there can be one or more non-linear layers, called hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://scikit-learn.org/stable/_images/multilayerperceptron_network.png\" width=\"300\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input or Visible Layers**\n",
    "\n",
    "This layer accepts input features. It provides information from the outside world to the network, no computation is performed at this layer, nodes here just pass on the information(features) to the hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hidden Layers**\n",
    "\n",
    "Nodes of this layer are not exposed to the outer world, they are the part of the abstraction provided by any neural network. Hidden layer performs all sort of computation on the features entered through the input layer and transfer the result to the output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output Layer**\n",
    "\n",
    "The final hidden layer is called the output layer and it is responsible for outputting a value or vector of values that correspond to the format required for the problem. The choice of activation function in he output layer is strongly constrained by the type of problem that you are modeling. For example: \n",
    ">* A regression problem may have a single output neuron and the neuron may have no activation function.\n",
    ">* A binary classification problem may have a single output neuron and use a sigmoid activation function to output a value between 0 and 1 to represent the probability of predicting a value for the class 1. This can be turned into a crisp class value by using a threshold of 0.5 and snap values less than the threshold to 0 otherwise to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are basically three steps in the training of the model.\n",
    "1. Forward pass\n",
    "2. Calculate error or loss\n",
    "3. Backward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Forward pass**\n",
    "\n",
    "In this step of training the model, we just pass the input to model and multiply with weights and add bias at every layer and find the calculated output of the model.\n",
    "![](https://miro.medium.com/max/1225/1*9dByklf9ybdvVtHq6RrOgw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Loss Calculate**\n",
    "\n",
    "When we pass the data instance(or one example) we will get some output from the model that is called Predicted output(pred_out) and we have the label with the data that is real output or expected output(Expect_out). Based upon these both we calculate the loss that we have to backpropagate(using Backpropagation algorithm). There is various Loss Function that we use based on our output and requirement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Backward Pass**\n",
    "\n",
    "After calculating the loss, we backpropagate the loss and updates the weights of the model by using gradient. This is the main step in the training of the model. In this step, weights will adjust according to the gradient flow in that direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/max/770/1*vGj29ZBD1kH1kDlGQspPxA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of Y can be anything ranging from -inf to +inf. The neuron really doesn’t know the bounds of the value. We decided to add “activation functions” for this purpose. To check the Y value produced by a neuron and decide whether outside connections should consider this neuron as “fired” or not. Or rather let’s say — “activated” or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/max/1225/1*p_hyqAtyI8pbt2kEl6siOQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Backpropagation](https://www.youtube.com/watch?v=Ilg3gGewQ5U&t=44s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Training an MLP with Tensorflow "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://github.com/smoreira/MultiLayerPerceptron/raw/master/imagem_mlp.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import and parse the dataset.\n",
    "2. Select the type of model.\n",
    "3. Train the model.\n",
    "4. Evaluate the model's effectiveness.\n",
    "5. Use the trained model to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure imports\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import and parse the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.9</td>\n",
       "      <td>2.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>5.5</td>\n",
       "      <td>2.6</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>5.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>4.4</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>5.5</td>\n",
       "      <td>2.4</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal_length  sepal_width  petal_length  petal_width  labels\n",
       "0             6.4          2.8           5.6          2.2       2\n",
       "1             5.0          2.3           3.3          1.0       1\n",
       "2             4.9          2.5           4.5          1.7       2\n",
       "3             4.9          3.1           1.5          0.1       0\n",
       "4             5.7          3.8           1.7          0.3       0\n",
       "..            ...          ...           ...          ...     ...\n",
       "115           5.5          2.6           4.4          1.2       1\n",
       "116           5.7          3.0           4.2          1.2       1\n",
       "117           4.4          2.9           1.4          0.2       0\n",
       "118           4.8          3.0           1.4          0.1       0\n",
       "119           5.5          2.4           3.7          1.0       1\n",
       "\n",
       "[120 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get data from url\n",
    "raw_data = pd.read_csv('https://storage.googleapis.com/download.tensorflow.org/data/iris_training.csv')\n",
    "columns_name = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'labels']\n",
    "raw_data.columns = columns_name\n",
    "raw_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each label is associated with string name (for example, \"setosa\"), but machine learning typically relies on numeric values. The label numbers are mapped to a named representation, such as:\n",
    "\n",
    ">* **0**: Iris setosa\n",
    ">* **1**: Iris versicolor\n",
    ">* **2**: Iris virginica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.4, 2.8, 5.6, 2.2],\n",
       "       [5. , 2.3, 3.3, 1. ],\n",
       "       [4.9, 2.5, 4.5, 1.7],\n",
       "       [4.9, 3.1, 1.5, 0.1],\n",
       "       [5.7, 3.8, 1.7, 0.3]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = raw_data.iloc[:, :-1].values\n",
    "features[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create labels for model\n",
    "labels = np.zeros((len(features), 3), dtype=np.int8)\n",
    "for i in raw_data['labels'].index:\n",
    "    labels[i][raw_data['labels'][i]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(features, raw_data['labels'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Select the type of model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.tensorflow.org/images/custom_estimators/full_network.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(4,))\n",
    "dense = layers.Dense(10, activation=\"relu\")\n",
    "x = dense(inputs)\n",
    "x = layers.Dense(10, activation=\"relu\")(x)\n",
    "outputs = layers.Dense(3)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Model(inputs=inputs, outputs=outputs, name=\"mnist_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"mnist_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         [(None, 4)]               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 10)                50        \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 3)                 33        \n",
      "=================================================================\n",
      "Total params: 193\n",
      "Trainable params: 193\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.2448 - accuracy: 0.1458\n",
      "Epoch 2/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.1684 - accuracy: 0.2604\n",
      "Epoch 3/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.1124 - accuracy: 0.2708\n",
      "Epoch 4/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.1040 - accuracy: 0.3229\n",
      "Epoch 5/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 1.0670 - accuracy: 0.3229\n",
      "Epoch 6/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.0514 - accuracy: 0.3646\n",
      "Epoch 7/200\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 1.0391 - accuracy: 0.3229\n",
      "Epoch 8/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.0185 - accuracy: 0.3854\n",
      "Epoch 9/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.9957 - accuracy: 0.3333\n",
      "Epoch 10/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.9963 - accuracy: 0.3333\n",
      "Epoch 11/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.9700 - accuracy: 0.3229\n",
      "Epoch 12/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.9483 - accuracy: 0.3333\n",
      "Epoch 13/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.9554 - accuracy: 0.3542\n",
      "Epoch 14/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.9355 - accuracy: 0.3333\n",
      "Epoch 15/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.9094 - accuracy: 0.5000\n",
      "Epoch 16/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.8955 - accuracy: 0.3646\n",
      "Epoch 17/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.8900 - accuracy: 0.3854\n",
      "Epoch 18/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.8716 - accuracy: 0.4167\n",
      "Epoch 19/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.8652 - accuracy: 0.4479\n",
      "Epoch 20/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.8477 - accuracy: 0.6042\n",
      "Epoch 21/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.8277 - accuracy: 0.6458\n",
      "Epoch 22/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.8165 - accuracy: 0.7396\n",
      "Epoch 23/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.7959 - accuracy: 0.7500\n",
      "Epoch 24/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7867 - accuracy: 0.7500\n",
      "Epoch 25/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7686 - accuracy: 0.7292\n",
      "Epoch 26/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.7647 - accuracy: 0.7917\n",
      "Epoch 27/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7464 - accuracy: 0.7396\n",
      "Epoch 28/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7423 - accuracy: 0.7708\n",
      "Epoch 29/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7174 - accuracy: 0.8125\n",
      "Epoch 30/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7084 - accuracy: 0.7604\n",
      "Epoch 31/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6908 - accuracy: 0.7708\n",
      "Epoch 32/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6769 - accuracy: 0.8542\n",
      "Epoch 33/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6638 - accuracy: 0.8021\n",
      "Epoch 34/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.6530 - accuracy: 0.7292\n",
      "Epoch 35/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6406 - accuracy: 0.8333\n",
      "Epoch 36/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6516 - accuracy: 0.8229\n",
      "Epoch 37/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6207 - accuracy: 0.7396\n",
      "Epoch 38/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6107 - accuracy: 0.8646\n",
      "Epoch 39/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5978 - accuracy: 0.8229\n",
      "Epoch 40/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6043 - accuracy: 0.8229\n",
      "Epoch 41/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.5864 - accuracy: 0.8438\n",
      "Epoch 42/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5818 - accuracy: 0.8646\n",
      "Epoch 43/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5629 - accuracy: 0.8021\n",
      "Epoch 44/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5539 - accuracy: 0.8542\n",
      "Epoch 45/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5484 - accuracy: 0.8854\n",
      "Epoch 46/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5472 - accuracy: 0.8125\n",
      "Epoch 47/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.5388 - accuracy: 0.8333\n",
      "Epoch 48/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.5399 - accuracy: 0.8229\n",
      "Epoch 49/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.5274 - accuracy: 0.8542\n",
      "Epoch 50/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.5097 - accuracy: 0.9167\n",
      "Epoch 51/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5126 - accuracy: 0.8750\n",
      "Epoch 52/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5082 - accuracy: 0.8854\n",
      "Epoch 53/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.5002 - accuracy: 0.8646\n",
      "Epoch 54/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4851 - accuracy: 0.8646\n",
      "Epoch 55/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.4887 - accuracy: 0.9271\n",
      "Epoch 56/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.4751 - accuracy: 0.8021\n",
      "Epoch 57/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4673 - accuracy: 0.9375\n",
      "Epoch 58/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4606 - accuracy: 0.9271\n",
      "Epoch 59/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4613 - accuracy: 0.8542\n",
      "Epoch 60/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4532 - accuracy: 0.9062\n",
      "Epoch 61/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.4457 - accuracy: 0.9271\n",
      "Epoch 62/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4507 - accuracy: 0.8958\n",
      "Epoch 63/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.4369 - accuracy: 0.9271\n",
      "Epoch 64/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4416 - accuracy: 0.8958\n",
      "Epoch 65/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.4472 - accuracy: 0.8438\n",
      "Epoch 66/200\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.4403 - accuracy: 0.8750\n",
      "Epoch 67/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4186 - accuracy: 0.9167\n",
      "Epoch 68/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.4186 - accuracy: 0.9375\n",
      "Epoch 69/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4120 - accuracy: 0.9479\n",
      "Epoch 70/200\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.4303 - accuracy: 0.8438\n",
      "Epoch 71/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4052 - accuracy: 0.9479\n",
      "Epoch 72/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4005 - accuracy: 0.8958\n",
      "Epoch 73/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3952 - accuracy: 0.9375\n",
      "Epoch 74/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.3935 - accuracy: 0.9062\n",
      "Epoch 75/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3902 - accuracy: 0.9583\n",
      "Epoch 76/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.3992 - accuracy: 0.8958\n",
      "Epoch 77/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3852 - accuracy: 0.8854\n",
      "Epoch 78/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3792 - accuracy: 0.9583\n",
      "Epoch 79/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3763 - accuracy: 0.9167\n",
      "Epoch 80/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.3719 - accuracy: 0.9479\n",
      "Epoch 81/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3694 - accuracy: 0.9479\n",
      "Epoch 82/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3768 - accuracy: 0.9167\n",
      "Epoch 83/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3697 - accuracy: 0.9167\n",
      "Epoch 84/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.3633 - accuracy: 0.9375\n",
      "Epoch 85/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3624 - accuracy: 0.9479\n",
      "Epoch 86/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3583 - accuracy: 0.9271\n",
      "Epoch 87/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.3650 - accuracy: 0.9167\n",
      "Epoch 88/200\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.3609 - accuracy: 0.8854\n",
      "Epoch 89/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3517 - accuracy: 0.9271\n",
      "Epoch 90/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.3514 - accuracy: 0.9271\n",
      "Epoch 91/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3476 - accuracy: 0.9688\n",
      "Epoch 92/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3671 - accuracy: 0.9062\n",
      "Epoch 93/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3447 - accuracy: 0.8750\n",
      "Epoch 94/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3404 - accuracy: 0.9271\n",
      "Epoch 95/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.3417 - accuracy: 0.9479\n",
      "Epoch 96/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.3304 - accuracy: 0.9479\n",
      "Epoch 97/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3277 - accuracy: 0.9583\n",
      "Epoch 98/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3352 - accuracy: 0.9479\n",
      "Epoch 99/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.3272 - accuracy: 0.9479\n",
      "Epoch 100/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3250 - accuracy: 0.9583\n",
      "Epoch 101/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3259 - accuracy: 0.9375\n",
      "Epoch 102/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3260 - accuracy: 0.9062\n",
      "Epoch 103/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3171 - accuracy: 0.9479\n",
      "Epoch 104/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3141 - accuracy: 0.9479\n",
      "Epoch 105/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3129 - accuracy: 0.9375\n",
      "Epoch 106/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3129 - accuracy: 0.9375\n",
      "Epoch 107/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.3087 - accuracy: 0.9583\n",
      "Epoch 108/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3071 - accuracy: 0.9375\n",
      "Epoch 109/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3108 - accuracy: 0.9271\n",
      "Epoch 110/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3069 - accuracy: 0.9271\n",
      "Epoch 111/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2998 - accuracy: 0.9375\n",
      "Epoch 112/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3155 - accuracy: 0.8958\n",
      "Epoch 113/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2985 - accuracy: 0.9062\n",
      "Epoch 114/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3038 - accuracy: 0.9479\n",
      "Epoch 115/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3063 - accuracy: 0.9167\n",
      "Epoch 116/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.2967 - accuracy: 0.9583\n",
      "Epoch 117/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.2911 - accuracy: 0.9688\n",
      "Epoch 118/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2897 - accuracy: 0.9688\n",
      "Epoch 119/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.2944 - accuracy: 0.9062\n",
      "Epoch 120/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.3196 - accuracy: 0.8750\n",
      "Epoch 121/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.2859 - accuracy: 0.9479\n",
      "Epoch 122/200\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2697 - accuracy: 0.96 - 0s 3ms/step - loss: 0.2846 - accuracy: 0.9375\n",
      "Epoch 123/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2877 - accuracy: 0.9479\n",
      "Epoch 124/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2808 - accuracy: 0.9375\n",
      "Epoch 125/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3045 - accuracy: 0.9375\n",
      "Epoch 126/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2758 - accuracy: 0.9167\n",
      "Epoch 127/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2739 - accuracy: 0.9583\n",
      "Epoch 128/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2690 - accuracy: 0.9375\n",
      "Epoch 129/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2699 - accuracy: 0.9375\n",
      "Epoch 130/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2975 - accuracy: 0.9167\n",
      "Epoch 131/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2626 - accuracy: 0.9583\n",
      "Epoch 132/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2657 - accuracy: 0.9375\n",
      "Epoch 133/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2620 - accuracy: 0.9583\n",
      "Epoch 134/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2689 - accuracy: 0.9479\n",
      "Epoch 135/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2592 - accuracy: 0.9583\n",
      "Epoch 136/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.2616 - accuracy: 0.9583\n",
      "Epoch 137/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2711 - accuracy: 0.9375\n",
      "Epoch 138/200\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.2522 - accuracy: 0.9688\n",
      "Epoch 139/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.2667 - accuracy: 0.9271\n",
      "Epoch 140/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.2494 - accuracy: 0.9583\n",
      "Epoch 141/200\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.2481 - accuracy: 0.9479\n",
      "Epoch 142/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2452 - accuracy: 0.9688\n",
      "Epoch 143/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.2470 - accuracy: 0.9479\n",
      "Epoch 144/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2490 - accuracy: 0.9583\n",
      "Epoch 145/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2488 - accuracy: 0.9792\n",
      "Epoch 146/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.2417 - accuracy: 0.9479\n",
      "Epoch 147/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.2510 - accuracy: 0.9479\n",
      "Epoch 148/200\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.2394 - accuracy: 0.9792\n",
      "Epoch 149/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.2341 - accuracy: 0.9688\n",
      "Epoch 150/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2334 - accuracy: 0.9688\n",
      "Epoch 151/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.2334 - accuracy: 0.9688\n",
      "Epoch 152/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2423 - accuracy: 0.9479\n",
      "Epoch 153/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.2305 - accuracy: 0.9583\n",
      "Epoch 154/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2373 - accuracy: 0.9479\n",
      "Epoch 155/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2264 - accuracy: 0.9688\n",
      "Epoch 156/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.2308 - accuracy: 0.9479\n",
      "Epoch 157/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2273 - accuracy: 0.9583\n",
      "Epoch 158/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.2457 - accuracy: 0.9375\n",
      "Epoch 159/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2201 - accuracy: 0.9688\n",
      "Epoch 160/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2273 - accuracy: 0.9479\n",
      "Epoch 161/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.2390 - accuracy: 0.9375\n",
      "Epoch 162/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.2207 - accuracy: 0.9688\n",
      "Epoch 163/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.2148 - accuracy: 0.9688\n",
      "Epoch 164/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.2160 - accuracy: 0.9688\n",
      "Epoch 165/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.2130 - accuracy: 0.9688\n",
      "Epoch 166/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2131 - accuracy: 0.9792\n",
      "Epoch 167/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.2296 - accuracy: 0.9375\n",
      "Epoch 168/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2136 - accuracy: 0.9583\n",
      "Epoch 169/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2163 - accuracy: 0.9688\n",
      "Epoch 170/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2048 - accuracy: 0.9688\n",
      "Epoch 171/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2070 - accuracy: 0.9688\n",
      "Epoch 172/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2038 - accuracy: 0.9688\n",
      "Epoch 173/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2139 - accuracy: 0.9583\n",
      "Epoch 174/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2009 - accuracy: 0.9688\n",
      "Epoch 175/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2014 - accuracy: 0.9479\n",
      "Epoch 176/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1983 - accuracy: 0.9792\n",
      "Epoch 177/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2010 - accuracy: 0.9688\n",
      "Epoch 178/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.1986 - accuracy: 0.9792\n",
      "Epoch 179/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1944 - accuracy: 0.9583\n",
      "Epoch 180/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2073 - accuracy: 0.9583\n",
      "Epoch 181/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1920 - accuracy: 0.9688\n",
      "Epoch 182/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1918 - accuracy: 0.9688\n",
      "Epoch 183/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1898 - accuracy: 0.9688\n",
      "Epoch 184/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1931 - accuracy: 0.9583\n",
      "Epoch 185/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2132 - accuracy: 0.9062\n",
      "Epoch 186/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1881 - accuracy: 0.9688\n",
      "Epoch 187/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1855 - accuracy: 0.9792\n",
      "Epoch 188/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1854 - accuracy: 0.9792\n",
      "Epoch 189/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.1909 - accuracy: 0.9583\n",
      "Epoch 190/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.2004 - accuracy: 0.9479\n",
      "Epoch 191/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.2014 - accuracy: 0.9479\n",
      "Epoch 192/200\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.1824 - accuracy: 0.9688\n",
      "Epoch 193/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.1871 - accuracy: 0.9688\n",
      "Epoch 194/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1781 - accuracy: 0.9479\n",
      "Epoch 195/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.1831 - accuracy: 0.9896\n",
      "Epoch 196/200\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.1834 - accuracy: 0.9688\n",
      "Epoch 197/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.1752 - accuracy: 0.9792\n",
      "Epoch 198/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.1811 - accuracy: 0.9583\n",
      "Epoch 199/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.1770 - accuracy: 0.9792\n",
      "Epoch 200/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.1751 - accuracy: 0.9688\n",
      "1/1 - 0s - loss: 0.2371 - accuracy: 0.9167\n",
      "Test loss: 0.23706169426441193\n",
      "Test accuracy: 0.9166666865348816\n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=keras.optimizers.SGD(learning_rate=0.01),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=32, epochs=200)\n",
    "\n",
    "test_scores = model.evaluate(x_test, y_test, verbose=2)\n",
    "print(\"Test loss:\", test_scores[0])\n",
    "print(\"Test accuracy:\", test_scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>30</th>\n",
       "      <th>4</th>\n",
       "      <th>setosa</th>\n",
       "      <th>versicolor</th>\n",
       "      <th>virginica</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.2</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.2</td>\n",
       "      <td>2.9</td>\n",
       "      <td>4.3</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5.5</td>\n",
       "      <td>4.2</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.1</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6.7</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>7.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.9</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5.6</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.9</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5.5</td>\n",
       "      <td>2.3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5.7</td>\n",
       "      <td>2.6</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5.7</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.9</td>\n",
       "      <td>5.6</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>5.8</td>\n",
       "      <td>2.7</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>6.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>5.2</td>\n",
       "      <td>4.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.1</td>\n",
       "      <td>4.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.3</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.9</td>\n",
       "      <td>4.3</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     30    4  setosa  versicolor  virginica\n",
       "0   5.9  3.0     4.2         1.5          1\n",
       "1   6.9  3.1     5.4         2.1          2\n",
       "2   5.1  3.3     1.7         0.5          0\n",
       "3   6.0  3.4     4.5         1.6          1\n",
       "4   5.5  2.5     4.0         1.3          1\n",
       "5   6.2  2.9     4.3         1.3          1\n",
       "6   5.5  4.2     1.4         0.2          0\n",
       "7   6.3  2.8     5.1         1.5          2\n",
       "8   5.6  3.0     4.1         1.3          1\n",
       "9   6.7  2.5     5.8         1.8          2\n",
       "10  7.1  3.0     5.9         2.1          2\n",
       "11  4.3  3.0     1.1         0.1          0\n",
       "12  5.6  2.8     4.9         2.0          2\n",
       "13  5.5  2.3     4.0         1.3          1\n",
       "14  6.0  2.2     4.0         1.0          1\n",
       "15  5.1  3.5     1.4         0.2          0\n",
       "16  5.7  2.6     3.5         1.0          1\n",
       "17  4.8  3.4     1.9         0.2          0\n",
       "18  5.1  3.4     1.5         0.2          0\n",
       "19  5.7  2.5     5.0         2.0          2\n",
       "20  5.4  3.4     1.7         0.2          0\n",
       "21  5.6  3.0     4.5         1.5          1\n",
       "22  6.3  2.9     5.6         1.8          2\n",
       "23  6.3  2.5     4.9         1.5          1\n",
       "24  5.8  2.7     3.9         1.2          1\n",
       "25  6.1  3.0     4.6         1.4          1\n",
       "26  5.2  4.1     1.5         0.1          0\n",
       "27  6.7  3.1     4.7         1.5          1\n",
       "28  6.7  3.3     5.7         2.5          2\n",
       "29  6.4  2.9     4.3         1.3          1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.read_csv(\"https://storage.googleapis.com/download.tensorflow.org/data/iris_test.csv\")\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = test_data.iloc[:, :-1].values\n",
    "labels_test = test_data.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1799 - accuracy: 0.9667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.17987222969532013, 0.9666666388511658]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(data_test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      1.0\n",
       "1      1.0\n",
       "2      NaN\n",
       "3      1.0\n",
       "4      1.0\n",
       "      ... \n",
       "115    1.0\n",
       "116    1.0\n",
       "117    1.0\n",
       "118    1.0\n",
       "119    1.0\n",
       "Name: labels, Length: 120, dtype: float64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data['labels'][2] = None\n",
    "raw_data['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data['labels'].fillna(raw_data['labels'].mean(), inplace=True)\n",
    "raw_data['labels'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in data.columns:\n",
    "    if ? != ?:\n",
    "        data[column].?(?, inplace=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
